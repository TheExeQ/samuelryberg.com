# Understanding AI

I wanted to better understand how AI models are created. Not to become an expert, but to gain an appreciation for the abstractions I use every day.

This post will highlight what I’ve learned so far. It’s written for other engineers who are new to topics like neural networks, deep learning, and transformers.

## Machine Learning

Software is _deterministic_. Given some input, if you run the program again, you will get the same output. A developer has explicitly written code to handle each case.

Most AI models are not this way — they are _probabilistic_. Developers don’t have to explicitly program the instructions.

[Machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) teaches software to recognize patterns from data. Given some input, you might _not_ get the same output. AI models like GPT (from OpenAI), Claude (from Anthropic), and Gemini (from Google) are “trained” on a large chunk of internet documents. These models learn patterns during training.

Then, there’s an API or chat interface where you can talk to the model. Based on some input, it can predict and generate sentences, images, or audio as output. You can think about machine learning as a subset of the broader AI category.

## Neural Networks

AI models are built on [neural networks](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) — think of them as a giant web of decision-making pathways that learn from examples. Neural networks can be used for anything, but I'll focus on language models.

These networks consist of layers of interconnected _neurons_ that process information:

1.  An **input layer** where data enters the system. Input is converted into a numerical representation of words or _tokens_ (more on tokens later).
2.  Many **hidden layers** that create an understanding of patterns in the system. Neurons inside the layer apply _weights_ (also known as **parameters**) to the input data and pass the result through an **activation function**. This function outputs a value, often between 0 and 1, representing the neuron's level of activation.
3.  An **output layer** which produces the final result, such as predicting the next word in a sentence. The outputs at this stage are often referred to as _logits_, which are raw scores that get transformed into probabilities.

For example, if the input was “San”, there is likely an activation close to 1 for the next word of “Francisco”. A unrelated word like “kitten” would be close to 0.

A big takeaway for me is: it’s just math. You can build a neural network [from first principles](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) using linear algebra, calculus, and statistics. You likely _won’t_ do this when there’s helpful abstractions like PyTorch, but it’s helpful for me to demystify what is happening under the hood.

## Deep Learning

Deep learning is a subset of machine learning that involves neural networks with _many_ layers—hence the term "deep." While a simple neural network may have just one or two hidden layers, deep learning models can have hundreds or even thousands of layers.

These additional layers enable the network to learn complex patterns. For example, large language models have been trained on multi-language datasets. This allows them to understand, generate, and translate text in multiple languages.

You can ask a question in English, and get a response back in Japanese.